import pandas as pd
import sqlite3
import numpy as np
from collections import defaultdict, deque
import os
import random

def get_important_nodes_by_community(csv_file_path, db_file_path, output_file_path=None):
    """
    ê° ì»¤ë®¤ë‹ˆí‹°ë³„ë¡œ degree ìƒìœ„ 10%ì˜ ì¤‘ìš” ë…¸ë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ë‹¨, ê° ì»¤ë®¤ë‹ˆí‹°ë§ˆë‹¤ ìµœì†Œ í•˜ë‚˜ì˜ ë…¸ë“œëŠ” í¬í•¨ë©ë‹ˆë‹¤.
    
    Args:
        csv_file_path (str): network_nodes.csv íŒŒì¼ ê²½ë¡œ
        db_file_path (str): subject_headings.db íŒŒì¼ ê²½ë¡œ
        output_file_path (str, optional): ê²°ê³¼ë¥¼ ì €ì¥í•  CSV íŒŒì¼ ê²½ë¡œ
    
    Returns:
        pd.DataFrame: ì¤‘ìš” ë…¸ë“œ ì •ë³´ê°€ ë‹´ê¸´ ë°ì´í„°í”„ë ˆì„
    """
    
    # CSV íŒŒì¼ ì½ê¸°
    print("Loading network nodes data...")
    df = pd.read_csv(csv_file_path)
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    print("Connecting to database...")
    conn = sqlite3.connect(db_file_path)
    
    # ê° ì»¤ë®¤ë‹ˆí‹°ë³„ë¡œ ê·¸ë£¹í™”
    communities = df.groupby('community')
    
    important_nodes = []
    
    print(f"Processing {len(communities)} communities...")
    
    for community_id, community_df in communities:
        # ì»¤ë®¤ë‹ˆí‹° ë‚´ ë…¸ë“œ ê°œìˆ˜
        total_nodes = len(community_df)
        
        # ìƒìœ„ 10% ê³„ì‚° (ìµœì†Œ 1ê°œ)
        top_5_percent_count = max(1, int(np.ceil(total_nodes * 0.1)))
        
        # degree ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬í•˜ì—¬ ìƒìœ„ ë…¸ë“œ ì„ íƒ
        top_nodes = community_df.nlargest(top_5_percent_count, 'degree')
        
        # ê° ë…¸ë“œì— ëŒ€í•´ ë¼ë²¨ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        for _, node_row in top_nodes.iterrows():
            doc_id = node_row['doc_id']
            
            # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¼ë²¨ ì¡°íšŒ
            cursor = conn.execute(
                "SELECT label FROM subjects WHERE node_id = ?",
                (doc_id,)
            )
            result = cursor.fetchone()
            
            if result:
                label = result[0]
                important_nodes.append({
                    'community': community_id,
                    'node_id': node_row['node_id'],
                    'doc_id': doc_id,
                    'degree': node_row['degree'],
                    'label': label,
                    'total_community_nodes': total_nodes,
                    'selected_nodes_count': top_5_percent_count,
                    'rank_in_community': top_nodes.index.get_loc(node_row.name) + 1
                })
            else:
                # ë¼ë²¨ì´ ì—†ëŠ” ê²½ìš°ë„ í¬í•¨
                important_nodes.append({
                    'community': community_id,
                    'node_id': node_row['node_id'],
                    'doc_id': doc_id,
                    'degree': node_row['degree'],
                    'label': None,
                    'total_community_nodes': total_nodes,
                    'selected_nodes_count': top_5_percent_count,
                    'rank_in_community': top_nodes.index.get_loc(node_row.name) + 1
                })
    
    conn.close()
    
    # ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    result_df = pd.DataFrame(important_nodes)
    
    # ì»¤ë®¤ë‹ˆí‹°ë³„ë¡œ ì •ë ¬
    result_df = result_df.sort_values(['community', 'rank_in_community'])
    
    print(f"Total important nodes selected: {len(result_df)}")
    print(f"Communities processed: {result_df['community'].nunique()}")
    
    # íŒŒì¼ë¡œ ì €ì¥ (ì˜µì…˜)
    if output_file_path:
        result_df.to_csv(output_file_path, index=False, encoding='utf-8')
        print(f"Results saved to: {output_file_path}")
    
    return result_df

def analyze_community_statistics(result_df):
    """
    ì»¤ë®¤ë‹ˆí‹°ë³„ í†µê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        result_df (pd.DataFrame): get_important_nodes_by_community ê²°ê³¼
    """
    print("\n=== Community Statistics ===")
    
    # ì»¤ë®¤ë‹ˆí‹°ë³„ í†µê³„
    community_stats = result_df.groupby('community').agg({
        'total_community_nodes': 'first',
        'selected_nodes_count': 'first',
        'degree': ['mean', 'max', 'min']
    }).round(2)
    
    community_stats.columns = ['total_nodes', 'selected_nodes', 'avg_degree', 'max_degree', 'min_degree']
    
    print(f"Top 10 largest communities:")
    top_communities = community_stats.nlargest(10, 'total_nodes')
    print(top_communities)
    
    print(f"\nOverall statistics:")
    print(f"- Total communities: {len(community_stats)}")
    print(f"- Average community size: {community_stats['total_nodes'].mean():.1f}")
    print(f"- Largest community size: {community_stats['total_nodes'].max()}")
    print(f"- Smallest community size: {community_stats['total_nodes'].min()}")
    print(f"- Total important nodes: {len(result_df)}")
    
    return community_stats

def analyze_node_count_distribution(result_df):
    """
    ì»¤ë®¤ë‹ˆí‹°ë³„ ì„ ì •ëœ ì¤‘ìš” ë…¸ë“œ ê°œìˆ˜ì˜ ë¶„í¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        result_df (pd.DataFrame): get_important_nodes_by_community ê²°ê³¼
    """
    print("\n=== Selected Node Count Distribution ===")
    
    # ì»¤ë®¤ë‹ˆí‹°ë³„ ì„ ì •ëœ ë…¸ë“œ ê°œìˆ˜ ê³„ì‚°
    node_count_per_community = result_df.groupby('community')['selected_nodes_count'].first()
    
    # ë¶„í¬ ë¶„ì„
    distribution = node_count_per_community.value_counts().sort_index()
    
    print("Distribution of selected nodes per community:")
    print("Selected Nodes | Communities | Percentage")
    print("-" * 42)
    
    total_communities = len(node_count_per_community)
    cumulative_count = 0
    
    for selected_count, community_count in distribution.items():
        cumulative_count += community_count
        percentage = (community_count / total_communities) * 100
        cumulative_percentage = (cumulative_count / total_communities) * 100
        print(f"{selected_count:12d} | {community_count:11d} | {percentage:8.2f}% (cumulative: {cumulative_percentage:.2f}%)")
    
    # í†µê³„ ìš”ì•½
    print(f"\nSummary Statistics:")
    print(f"- Mean selected nodes per community: {node_count_per_community.mean():.2f}")
    print(f"- Median selected nodes per community: {node_count_per_community.median():.2f}")
    print(f"- Min selected nodes per community: {node_count_per_community.min()}")
    print(f"- Max selected nodes per community: {node_count_per_community.max()}")
    print(f"- Standard deviation: {node_count_per_community.std():.2f}")
    
    # ì£¼ìš” ë¶„í¬ êµ¬ê°„ë³„ ë¶„ì„
    print(f"\nDistribution by ranges:")
    print(f"- Communities with 1 node: {(node_count_per_community == 1).sum()} ({((node_count_per_community == 1).sum() / total_communities * 100):.1f}%)")
    print(f"- Communities with 2-5 nodes: {((node_count_per_community >= 2) & (node_count_per_community <= 5)).sum()} ({(((node_count_per_community >= 2) & (node_count_per_community <= 5)).sum() / total_communities * 100):.1f}%)")
    print(f"- Communities with 6-10 nodes: {((node_count_per_community >= 6) & (node_count_per_community <= 10)).sum()} ({(((node_count_per_community >= 6) & (node_count_per_community <= 10)).sum() / total_communities * 100):.1f}%)")
    print(f"- Communities with 11-20 nodes: {((node_count_per_community >= 11) & (node_count_per_community <= 20)).sum()} ({(((node_count_per_community >= 11) & (node_count_per_community <= 20)).sum() / total_communities * 100):.1f}%)")
    print(f"- Communities with 21+ nodes: {(node_count_per_community >= 21).sum()} ({((node_count_per_community >= 21).sum() / total_communities * 100):.1f}%)")
    
    return node_count_per_community

def analyze_distance_to_important_nodes(db_file_path, result_df, sample_size=1000, max_distance=6):
    """
    ëœë¤í•˜ê²Œ ì„ ì •ëœ ë…¸ë“œì—ì„œ ì¤‘ìš” ë…¸ë“œê¹Œì§€ì˜ ìµœë‹¨ ê±°ë¦¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        db_file_path (str): subject_headings.db íŒŒì¼ ê²½ë¡œ
        result_df (pd.DataFrame): ì¤‘ìš” ë…¸ë“œ ì •ë³´ê°€ ë‹´ê¸´ ë°ì´í„°í”„ë ˆì„
        sample_size (int): ë¶„ì„í•  ëœë¤ ë…¸ë“œ ê°œìˆ˜
        max_distance (int): íƒìƒ‰í•  ìµœëŒ€ ê±°ë¦¬
    
    Returns:
        dict: ê±°ë¦¬ ë¶„ì„ ê²°ê³¼
    """
    print(f"\n=== Distance Analysis to Important Nodes ===")
    print(f"Analyzing {sample_size} random nodes...")
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    conn = sqlite3.connect(db_file_path)
    
    # ì¤‘ìš” ë…¸ë“œ ì§‘í•© ìƒì„±
    important_node_ids = set(result_df['doc_id'].tolist())
    print(f"Important nodes count: {len(important_node_ids)}")
    
    # ì „ì²´ ë…¸ë“œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    cursor = conn.execute("SELECT node_id FROM subjects")
    all_nodes = [row[0] for row in cursor.fetchall()]
    print(f"Total nodes in network: {len(all_nodes)}")
    
    # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ êµ¬ì¶• (ì¸ì ‘ ë¦¬ìŠ¤íŠ¸)
    print("Building network graph...")
    graph = defaultdict(list)
    
    # ëª¨ë“  ê´€ê³„ë¥¼ ë¬´ë°©í–¥ ê·¸ë˜í”„ë¡œ êµ¬ì¶• (ê´€ê³„ ìœ í˜• ìƒê´€ì—†ì´)
    cursor = conn.execute("SELECT source_id, target_id FROM relations")
    edge_count = 0
    for source, target in cursor:
        graph[source].append(target)
        graph[target].append(source)  # ë¬´ë°©í–¥ ê·¸ë˜í”„
        edge_count += 1
        if edge_count % 500000 == 0:
            print(f"  Processed {edge_count} edges...")
    
    print(f"Graph built with {edge_count} edges")
    
    # ëœë¤ ë…¸ë“œ ìƒ˜í”Œë§
    random.seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•´
    sample_nodes = random.sample(all_nodes, min(sample_size, len(all_nodes)))
    
    distances = []
    unreachable_count = 0
    
    print("Calculating distances...")
    for i, start_node in enumerate(sample_nodes):
        if (i + 1) % 100 == 0:
            print(f"  Processed {i + 1}/{len(sample_nodes)} nodes...")
        
        # BFSë¡œ ìµœë‹¨ ê±°ë¦¬ ê³„ì‚°
        distance = bfs_distance_to_important_nodes(graph, start_node, important_node_ids, max_distance)
        
        if distance == -1:
            unreachable_count += 1
        else:
            distances.append(distance)
    
    conn.close()
    
    # ê²°ê³¼ ë¶„ì„
    if distances:
        print(f"\n=== Distance Analysis Results ===")
        print(f"Sample size: {len(sample_nodes)}")
        print(f"Reachable nodes: {len(distances)}")
        print(f"Unreachable nodes (within {max_distance} steps): {unreachable_count}")
        print(f"Reachability rate: {(len(distances) / len(sample_nodes)) * 100:.2f}%")
        
        distances_array = np.array(distances)
        
        print(f"\nDistance Statistics:")
        print(f"- Mean distance: {distances_array.mean():.2f}")
        print(f"- Median distance: {np.median(distances_array):.2f}")
        print(f"- Min distance: {distances_array.min()}")
        print(f"- Max distance: {distances_array.max()}")
        print(f"- Standard deviation: {distances_array.std():.2f}")
        
        print(f"\nDistance Distribution:")
        distance_counts = np.bincount(distances_array)
        for distance, count in enumerate(distance_counts):
            if count > 0:
                percentage = (count / len(distances)) * 100
                print(f"- Distance {distance}: {count} nodes ({percentage:.1f}%)")
        
        return {
            'distances': distances,
            'unreachable_count': unreachable_count,
            'sample_size': len(sample_nodes),
            'reachability_rate': (len(distances) / len(sample_nodes)) * 100,
            'mean_distance': distances_array.mean(),
            'median_distance': np.median(distances_array)
        }
    else:
        print("No reachable important nodes found within the specified distance limit.")
        return None

def bfs_distance_to_important_nodes(graph, start_node, important_nodes, max_distance):
    """
    BFSë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œì‘ ë…¸ë“œì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ì¤‘ìš” ë…¸ë“œê¹Œì§€ì˜ ê±°ë¦¬ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        graph (dict): ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ (ì¸ì ‘ ë¦¬ìŠ¤íŠ¸)
        start_node (str): ì‹œì‘ ë…¸ë“œ ID
        important_nodes (set): ì¤‘ìš” ë…¸ë“œ ID ì§‘í•©
        max_distance (int): ìµœëŒ€ íƒìƒ‰ ê±°ë¦¬
    
    Returns:
        int: ìµœë‹¨ ê±°ë¦¬ (-1 if unreachable within max_distance)
    """
    if start_node in important_nodes:
        return 0
    
    visited = {start_node}
    queue = deque([(start_node, 0)])
    
    while queue:
        current_node, distance = queue.popleft()
        
        if distance >= max_distance:
            continue
        
        for neighbor in graph[current_node]:
            if neighbor not in visited:
                if neighbor in important_nodes:
                    return distance + 1
                
                visited.add(neighbor)
                queue.append((neighbor, distance + 1))
    
    return -1  # ìµœëŒ€ ê±°ë¦¬ ë‚´ì—ì„œ ì¤‘ìš” ë…¸ë“œì— ë„ë‹¬í•  ìˆ˜ ì—†ìŒ

def find_connected_important_node_pairs(db_file_path, result_df, output_file_path=None):
    """
    ì¤‘ìš” ë…¸ë“œë“¤ ì¤‘ì—ì„œ ì„œë¡œ ì—°ê²°ëœ ìŒë“¤ì„ ì°¾ìŠµë‹ˆë‹¤.
    
    Args:
        db_file_path (str): subject_headings.db íŒŒì¼ ê²½ë¡œ
        result_df (pd.DataFrame): ì¤‘ìš” ë…¸ë“œ ì •ë³´ê°€ ë‹´ê¸´ ë°ì´í„°í”„ë ˆì„
        output_file_path (str, optional): ê²°ê³¼ë¥¼ ì €ì¥í•  CSV íŒŒì¼ ê²½ë¡œ
    
    Returns:
        pd.DataFrame: ì—°ê²°ëœ ì¤‘ìš” ë…¸ë“œ ìŒë“¤ì˜ ì •ë³´
    """
    print(f"\n=== Finding Connected Important Node Pairs ===")
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    conn = sqlite3.connect(db_file_path)
    
    # ì¤‘ìš” ë…¸ë“œ ì§‘í•© ìƒì„±
    important_node_ids = set(result_df['doc_id'].tolist())
    print(f"Important nodes count: {len(important_node_ids)}")
    
    # ì¤‘ìš” ë…¸ë“œ ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (ë¹ ë¥¸ ê²€ìƒ‰ì„ ìœ„í•´)
    node_info_dict = {}
    for _, row in result_df.iterrows():
        node_info_dict[row['doc_id']] = {
            'label': row['label'],
            'community': row['community'],
            'degree': row['degree'],
            'rank_in_community': row['rank_in_community']
        }
    
    # ì—°ê²°ëœ ìŒë“¤ ì°¾ê¸°
    connected_pairs = []
    processed_pairs = set()  # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•œ ì§‘í•©
    
    print("Searching for connections between important nodes...")
    
    # ì¤‘ìš” ë…¸ë“œë“¤ ê°„ì˜ ì§ì ‘ ì—°ê²° ì°¾ê¸°
    cursor = conn.execute("""
        SELECT source_id, target_id, relation_type 
        FROM relations 
        WHERE source_id IN ({}) AND target_id IN ({})
    """.format(
        ','.join(['?' for _ in important_node_ids]),
        ','.join(['?' for _ in important_node_ids])
    ), list(important_node_ids) + list(important_node_ids))
    
    connection_count = 0
    for source_id, target_id, relation_type in cursor:
        # ë¬´ë°©í–¥ ê·¸ë˜í”„ë¡œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ì •ë ¬ëœ ìˆœì„œë¡œ ìŒ ìƒì„±
        pair_key = tuple(sorted([source_id, target_id]))
        
        if pair_key not in processed_pairs and source_id != target_id:
            processed_pairs.add(pair_key)
            
            source_info = node_info_dict[source_id]
            target_info = node_info_dict[target_id]
            
            # ê°™ì€ ì»¤ë®¤ë‹ˆí‹°ì¸ì§€ í™•ì¸
            same_community = source_info['community'] == target_info['community']
            
            connected_pairs.append({
                'source_id': source_id,
                'source_label': source_info['label'],
                'source_community': source_info['community'],
                'source_degree': source_info['degree'],
                'source_rank': source_info['rank_in_community'],
                'target_id': target_id,
                'target_label': target_info['label'],
                'target_community': target_info['community'],
                'target_degree': target_info['degree'],
                'target_rank': target_info['rank_in_community'],
                'relation_type': relation_type,
                'same_community': same_community,
                'community_distance': 0 if same_community else 1
            })
            
            connection_count += 1
            if connection_count % 1000 == 0:
                print(f"  Found {connection_count} connections...")
    
    conn.close()
    
    # ê²°ê³¼ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    if connected_pairs:
        pairs_df = pd.DataFrame(connected_pairs)
        
        # ê´€ê³„ ìœ í˜•ë³„ë¡œ ì •ë ¬
        pairs_df = pairs_df.sort_values(['relation_type', 'same_community'], ascending=[True, False])
        
        print(f"\n=== Connection Analysis Results ===")
        print(f"Total connected pairs: {len(pairs_df)}")
        
        # ê´€ê³„ ìœ í˜•ë³„ ë¶„í¬
        print(f"\nConnection types distribution:")
        relation_counts = pairs_df['relation_type'].value_counts()
        for relation_type, count in relation_counts.items():
            percentage = (count / len(pairs_df)) * 100
            print(f"- {relation_type}: {count} pairs ({percentage:.1f}%)")
        
        # ì»¤ë®¤ë‹ˆí‹° ë‚´/ê°„ ì—°ê²° ë¶„í¬
        print(f"\nCommunity connection distribution:")
        same_community_count = pairs_df['same_community'].sum()
        inter_community_count = len(pairs_df) - same_community_count
        print(f"- Same community: {same_community_count} pairs ({(same_community_count / len(pairs_df)) * 100:.1f}%)")
        print(f"- Inter-community: {inter_community_count} pairs ({(inter_community_count / len(pairs_df)) * 100:.1f}%)")
        
        # ê°€ì¥ ë§ì´ ì—°ê²°ëœ ë…¸ë“œë“¤
        print(f"\nMost connected important nodes:")
        node_connections = defaultdict(int)
        for _, row in pairs_df.iterrows():
            node_connections[row['source_id']] += 1
            node_connections[row['target_id']] += 1
        
        # ìƒìœ„ 10ê°œ ë…¸ë“œ
        top_connected = sorted(node_connections.items(), key=lambda x: x[1], reverse=True)[:10]
        for i, (node_id, conn_count) in enumerate(top_connected, 1):
            node_info = node_info_dict[node_id]
            print(f"{i:2d}. {node_info['label']} (Community: {node_info['community']}) - {conn_count} connections")
        
        # ê´€ê³„ ìœ í˜•ë³„ ìƒìœ„ ìŒë“¤ ì¶œë ¥
        print(f"\nTop connections by relation type:")
        for relation_type in ['broader', 'narrower', 'related', 'cosine_related']:
            type_pairs = pairs_df[pairs_df['relation_type'] == relation_type]
            if len(type_pairs) > 0:
                print(f"\n{relation_type.upper()} ({len(type_pairs)} pairs):")
                for i, (_, row) in enumerate(type_pairs.head(3).iterrows(), 1):
                    community_marker = "ğŸ " if row['same_community'] else "ğŸŒ‰"
                    print(f"  {i}. {row['source_label']} â†” {row['target_label']} {community_marker}")
        
        # íŒŒì¼ë¡œ ì €ì¥ (ì˜µì…˜)
        if output_file_path:
            pairs_df.to_csv(output_file_path, index=False, encoding='utf-8')
            print(f"\nConnected pairs saved to: {output_file_path}")
        
        return pairs_df
    
    else:
        print("No direct connections found between important nodes.")
        return pd.DataFrame()

def analyze_connection_patterns(pairs_df, result_df):
    """
    ì—°ê²° íŒ¨í„´ì„ ì‹¬í™” ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        pairs_df (pd.DataFrame): ì—°ê²°ëœ ìŒ ì •ë³´
        result_df (pd.DataFrame): ì¤‘ìš” ë…¸ë“œ ì •ë³´
    """
    if len(pairs_df) == 0:
        print("No connections to analyze.")
        return
    
    print(f"\n=== Connection Pattern Analysis ===")
    
    # ì»¤ë®¤ë‹ˆí‹°ë³„ ë‚´ë¶€ ì—°ê²° ë°€ë„
    community_internal_connections = pairs_df[pairs_df['same_community'] == True].groupby('source_community').size()
    community_sizes = result_df.groupby('community').size()
    
    print(f"\nCommunities with highest internal connectivity:")
    community_density = {}
    for community in community_internal_connections.index:
        size = community_sizes.get(community, 0)
        connections = community_internal_connections[community]
        max_possible = size * (size - 1) / 2  # ì™„ì „ ê·¸ë˜í”„ì˜ ìµœëŒ€ ì—°ê²° ìˆ˜
        density = connections / max_possible if max_possible > 0 else 0
        community_density[community] = {
            'size': size,
            'connections': connections,
            'density': density
        }
    
    # ë°€ë„ ê¸°ì¤€ ìƒìœ„ 10ê°œ ì»¤ë®¤ë‹ˆí‹°
    top_dense_communities = sorted(community_density.items(), key=lambda x: x[1]['density'], reverse=True)[:10]
    for i, (community, stats) in enumerate(top_dense_communities, 1):
        if stats['density'] > 0:
            print(f"{i:2d}. Community {community}: {stats['connections']}/{int(stats['size']*(stats['size']-1)/2)} connections (density: {stats['density']:.3f})")
    
    # ë¸Œë¦¿ì§€ ì—­í• ì„ í•˜ëŠ” ë…¸ë“œë“¤ (ì—¬ëŸ¬ ì»¤ë®¤ë‹ˆí‹°ì™€ ì—°ê²°)
    bridge_nodes = defaultdict(set)
    for _, row in pairs_df[pairs_df['same_community'] == False].iterrows():
        bridge_nodes[row['source_id']].add(row['target_community'])
        bridge_nodes[row['target_id']].add(row['source_community'])
    
    print(f"\nTop bridge nodes (connecting multiple communities):")
    bridge_scores = {node: len(communities) for node, communities in bridge_nodes.items()}
    top_bridges = sorted(bridge_scores.items(), key=lambda x: x[1], reverse=True)[:10]
    
    node_info_dict = {row['doc_id']: row for _, row in result_df.iterrows()}
    for i, (node_id, bridge_score) in enumerate(top_bridges, 1):
        if bridge_score > 1:  # 2ê°œ ì´ìƒ ì»¤ë®¤ë‹ˆí‹°ì™€ ì—°ê²°
            node_info = node_info_dict[node_id]
            print(f"{i:2d}. {node_info['label']} (Community: {node_info['community']}) - connects to {bridge_score} other communities")

def create_balanced_node_pairs_sample(result_df, connected_pairs_df, total_sample_size=100000):
    """
    ì¤‘ìš” ë…¸ë“œë“¤ ì¤‘ì—ì„œ ì—°ê²°ëœ ìŒê³¼ ì—°ê²°ë˜ì§€ ì•Šì€ ìŒì„ ê· í˜•ìˆê²Œ ìƒ˜í”Œë§í•˜ì—¬ ì§€ì •ëœ í¬ê¸°ì˜ ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        result_df (pd.DataFrame): ì¤‘ìš” ë…¸ë“œ ì •ë³´
        connected_pairs_df (pd.DataFrame): ì—°ê²°ëœ ì¤‘ìš” ë…¸ë“œ ìŒë“¤
        total_sample_size (int): ìƒì„±í•  ì´ ìƒ˜í”Œ í¬ê¸°
    
    Returns:
        pd.DataFrame: ê· í˜•ì¡íŒ ë…¸ë“œ ìŒ ìƒ˜í”Œ
    """
    print(f"\n=== Creating Balanced Node Pairs Sample ({total_sample_size:,} pairs) ===")
    
    # ì¤‘ìš” ë…¸ë“œ ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
    node_info_dict = {}
    for _, row in result_df.iterrows():
        node_info_dict[row['doc_id']] = {
            'label': row['label'],
            'community': row['community'],
            'degree': row['degree'],
            'rank_in_community': row['rank_in_community']
        }
    
    important_nodes = list(result_df['doc_id'])
    print(f"Total important nodes: {len(important_nodes):,}")
    
    # ì—°ê²°ëœ ìŒë“¤ì˜ ì§‘í•© ìƒì„± (ë¹ ë¥¸ ê²€ìƒ‰ì„ ìœ„í•´)
    connected_pairs_set = set()
    for _, row in connected_pairs_df.iterrows():
        pair_key = tuple(sorted([row['source_id'], row['target_id']]))
        connected_pairs_set.add(pair_key)
    
    print(f"Connected pairs: {len(connected_pairs_set):,}")
    
    # ì—°ê²°ëœ ìŒë“¤ì„ ìƒ˜í”Œì— í¬í•¨
    connected_sample = []
    for _, row in connected_pairs_df.iterrows():
        source_info = node_info_dict[row['source_id']]
        target_info = node_info_dict[row['target_id']]
        
        connected_sample.append({
            'source_id': row['source_id'],
            'source_label': source_info['label'],
            'source_community': source_info['community'],
            'source_degree': source_info['degree'],
            'source_rank': source_info['rank_in_community'],
            'target_id': row['target_id'],
            'target_label': target_info['label'],
            'target_community': target_info['community'],
            'target_degree': target_info['degree'],
            'target_rank': target_info['rank_in_community'],
            'is_connected': True,
            'existing_relation_type': row['relation_type'],
            'same_community': row['same_community']
        })
    
    print(f"Added {len(connected_sample):,} connected pairs")
    
    # ì—°ê²°ë˜ì§€ ì•Šì€ ìŒë“¤ì„ ëœë¤ ìƒ˜í”Œë§
    remaining_sample_size = total_sample_size - len(connected_sample)
    print(f"Need to sample {remaining_sample_size:,} unconnected pairs")
    
    if remaining_sample_size <= 0:
        print("Connected pairs already exceed target sample size. Using only connected pairs.")
        sample_df = pd.DataFrame(connected_sample[:total_sample_size])
    else:
        # ì „ì²´ ê°€ëŠ¥í•œ ìŒì˜ ìˆ˜ ê³„ì‚°
        total_possible_pairs = len(important_nodes) * (len(important_nodes) - 1) // 2
        total_unconnected_pairs = total_possible_pairs - len(connected_pairs_set)
        
        print(f"Total possible pairs: {total_possible_pairs:,}")
        print(f"Total unconnected pairs: {total_unconnected_pairs:,}")
        
        # ëœë¤ ì‹œë“œ ì„¤ì •
        random.seed(42)
        
        # ì—°ê²°ë˜ì§€ ì•Šì€ ìŒë“¤ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìƒ˜í”Œë§
        unconnected_sample = []
        attempts = 0
        max_attempts = remaining_sample_size * 10  # ë¬´í•œ ë£¨í”„ ë°©ì§€
        
        print("Sampling unconnected pairs...")
        sample_progress_interval = max(1, remaining_sample_size // 20)  # 20íšŒ ì§„í–‰ë¥  í‘œì‹œ
        
        while len(unconnected_sample) < remaining_sample_size and attempts < max_attempts:
            # ëœë¤í•˜ê²Œ ë‘ ë…¸ë“œ ì„ íƒ
            source_idx = random.randint(0, len(important_nodes) - 1)
            target_idx = random.randint(0, len(important_nodes) - 1)
            
            if source_idx != target_idx:  # ìê¸° ìì‹ ê³¼ì˜ ìŒ ì œì™¸
                source_id = important_nodes[source_idx]
                target_id = important_nodes[target_idx]
                
                # ì •ë ¬ëœ ìˆœì„œë¡œ ìŒ ìƒì„±
                pair_key = tuple(sorted([source_id, target_id]))
                
                # ì—°ê²°ë˜ì§€ ì•Šì€ ìŒì¸ì§€ í™•ì¸
                if pair_key not in connected_pairs_set:
                    source_info = node_info_dict[source_id]
                    target_info = node_info_dict[target_id]
                    
                    # ê°™ì€ ì»¤ë®¤ë‹ˆí‹°ì¸ì§€ í™•ì¸
                    same_community = source_info['community'] == target_info['community']
                    
                    unconnected_sample.append({
                        'source_id': source_id,
                        'source_label': source_info['label'],
                        'source_community': source_info['community'],
                        'source_degree': source_info['degree'],
                        'source_rank': source_info['rank_in_community'],
                        'target_id': target_id,
                        'target_label': target_info['label'],
                        'target_community': target_info['community'],
                        'target_degree': target_info['degree'],
                        'target_rank': target_info['rank_in_community'],
                        'is_connected': False,
                        'existing_relation_type': None,
                        'same_community': same_community
                    })
                    
                    # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ì§‘í•©ì— ì¶”ê°€
                    connected_pairs_set.add(pair_key)
                    
                    # ì§„í–‰ë¥  í‘œì‹œ
                    if len(unconnected_sample) % sample_progress_interval == 0:
                        progress = (len(unconnected_sample) / remaining_sample_size) * 100
                        print(f"  Sampled {len(unconnected_sample):,}/{remaining_sample_size:,} unconnected pairs ({progress:.1f}%)")
            
            attempts += 1
        
        print(f"Successfully sampled {len(unconnected_sample):,} unconnected pairs")
        
        # ì—°ê²°ëœ ìŒê³¼ ì—°ê²°ë˜ì§€ ì•Šì€ ìŒì„ ê²°í•©
        all_samples = connected_sample + unconnected_sample
        sample_df = pd.DataFrame(all_samples)
    
    # ê²°ê³¼ ë¶„ì„
    print(f"\n=== Sample Dataset Analysis ===")
    print(f"Total sample size: {len(sample_df):,}")
    
    connected_count = sample_df['is_connected'].sum()
    unconnected_count = len(sample_df) - connected_count
    
    print(f"Connected pairs: {connected_count:,} ({(connected_count/len(sample_df)*100):.1f}%)")
    print(f"Unconnected pairs: {unconnected_count:,} ({(unconnected_count/len(sample_df)*100):.1f}%)")
    
    # ì»¤ë®¤ë‹ˆí‹° ë‚´/ê°„ ë¶„í¬
    same_community_count = sample_df['same_community'].sum()
    inter_community_count = len(sample_df) - same_community_count
    
    print(f"\nCommunity distribution:")
    print(f"Same community pairs: {same_community_count:,} ({(same_community_count/len(sample_df)*100):.1f}%)")
    print(f"Inter-community pairs: {inter_community_count:,} ({(inter_community_count/len(sample_df)*100):.1f}%)")
    
    # ì—°ê²°ëœ ìŒ ì¤‘ ê´€ê³„ ìœ í˜• ë¶„í¬
    if connected_count > 0:
        print(f"\nRelation types in connected pairs:")
        relation_counts = sample_df[sample_df['is_connected'] == True]['existing_relation_type'].value_counts()
        for relation_type, count in relation_counts.head(5).items():
            percentage = (count / connected_count) * 100
            print(f"- {relation_type}: {count:,} pairs ({percentage:.1f}%)")
    
    # ìƒ˜í”Œ ì˜ˆì‹œ ì¶œë ¥
    print(f"\nSample examples:")
    print("Connected pairs:")
    connected_examples = sample_df[sample_df['is_connected'] == True].head(3)
    for i, (_, row) in enumerate(connected_examples.iterrows(), 1):
        community_marker = "ğŸ " if row['same_community'] else "ğŸŒ‰"
        print(f"  {i}. {row['source_label']} â†” {row['target_label']} ({row['existing_relation_type']}) {community_marker}")
    
    print("Unconnected pairs:")
    unconnected_examples = sample_df[sample_df['is_connected'] == False].head(3)
    for i, (_, row) in enumerate(unconnected_examples.iterrows(), 1):
        community_marker = "ğŸ " if row['same_community'] else "ğŸŒ‰"
        print(f"  {i}. {row['source_label']} â†” {row['target_label']} (no connection) {community_marker}")
    
    return sample_df

def main():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    # íŒŒì¼ ê²½ë¡œ ì„¤ì •
    current_dir = os.path.dirname(os.path.abspath(__file__))
    print(current_dir)
    csv_file_path = os.path.join(current_dir,'..', "louvain_analysis_final", "network_nodes.csv")
    db_file_path = os.path.join(current_dir, "..", "..", "data", "subject_headings.db")
    output_file_path = os.path.join(current_dir, 'results', "important_nodes_by_community.csv")
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(csv_file_path):
        print(f"Error: CSV file not found at {csv_file_path}")
        return
    
    if not os.path.exists(db_file_path):
        print(f"Error: Database file not found at {db_file_path}")
        return
    
    # ì¤‘ìš” ë…¸ë“œ ì¶”ì¶œ
    result_df = get_important_nodes_by_community(csv_file_path, db_file_path, output_file_path)
    
    # í†µê³„ ë¶„ì„
    community_stats = analyze_community_statistics(result_df)
    
    # ë…¸ë“œ ê°œìˆ˜ ë¶„í¬ ë¶„ì„
    node_distribution = analyze_node_count_distribution(result_df)
    
    # ëœë¤ ë…¸ë“œì—ì„œ ì¤‘ìš” ë…¸ë“œê¹Œì§€ì˜ ê±°ë¦¬ ë¶„ì„
    distance_analysis = analyze_distance_to_important_nodes(db_file_path, result_df, sample_size=500, max_distance=5)
    
    # ì¤‘ìš” ë…¸ë“œë“¤ ê°„ì˜ ì—°ê²° ë¶„ì„
    connected_pairs_output = os.path.join(current_dir, "connected_important_pairs.csv")
    connected_pairs = find_connected_important_node_pairs(db_file_path, result_df, connected_pairs_output)
    
    # ì—°ê²° íŒ¨í„´ ì‹¬í™” ë¶„ì„
    if len(connected_pairs) > 0:
        analyze_connection_patterns(connected_pairs, result_df)
    
    # ê· í˜•ì¡íŒ ë…¸ë“œ ìŒ ìƒ˜í”Œ ìƒì„± (10ë§Œê°œ)
    balanced_sample_output = os.path.join(current_dir, "balanced_node_pairs_sample.csv")
    balanced_sample = create_balanced_node_pairs_sample(result_df, connected_pairs, total_sample_size=100000)
    balanced_sample.to_csv(balanced_sample_output, index=False, encoding='utf-8')
    print(f"\nBalanced sample saved to: {balanced_sample_output}")
    
    # ìƒ˜í”Œ ê²°ê³¼ ì¶œë ¥
    print("\n=== Sample Results ===")
    print("Top 5 communities by size with their important nodes:")
    for community_id in community_stats.nlargest(5, 'total_nodes').index:
        community_nodes = result_df[result_df['community'] == community_id]
        print(f"\nCommunity {community_id} (total nodes: {community_nodes.iloc[0]['total_community_nodes']}):")
        for _, node in community_nodes.head(3).iterrows():  # ìƒìœ„ 3ê°œë§Œ ì¶œë ¥
            print(f"  - Rank {node['rank_in_community']}: {node['label']} (degree: {node['degree']})")

if __name__ == "__main__":
    main()
